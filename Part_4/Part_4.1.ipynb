{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bbd1c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (18, 10)\n",
    "\n",
    "# OpenCV\n",
    "import cv2\n",
    "\n",
    "# Keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Cleverhans\n",
    "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method\n",
    "\n",
    "# Other modules\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b580f",
   "metadata": {},
   "source": [
    "Similar to what we did in Project 1, we will now define some constant for the path to annotations and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4378365",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path('../data')\n",
    "IMG_FOLDER = Path(DATA_FOLDER / 'tsrd-train')\n",
    "ANNOTATIONS = Path(DATA_FOLDER / 'TsignRecgTrain4170Annotation.txt')\n",
    "COL_NAMES = ['filename','width','height','x1','y1','x2','y2','label']\n",
    "MODEL_FOLDER = Path('../Part_1')\n",
    "MODEL = Path(MODEL_FOLDER / 'chinese_traffic_sign_classifier_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea16ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(ANNOTATIONS, names=COL_NAMES, sep=';', header=None, index_col=False)\n",
    "classifier = load_model(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c2b3c7",
   "metadata": {},
   "source": [
    "In order to create the adversarial examples, we will first import all the images. We use the same function calls from Project 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7fdae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob(str(IMG_FOLDER) + '/*.png')\n",
    "IMG_SIZE = (134,128)\n",
    "\n",
    "dataset = []\n",
    "labels = []\n",
    "\n",
    "for idx in range(len(images)):\n",
    "    full_img_path = images[idx]\n",
    "    filename = Path(full_img_path).name\n",
    "    img_bgr = cv2.imread(full_img_path)\n",
    "    image = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, IMG_SIZE)\n",
    "    image = img_to_array(image)\n",
    "    dataset.append(image)\n",
    "    # get label specific to this image\n",
    "    row = df[df['filename'] == filename]\n",
    "    label = row['label'].values[0]\n",
    "    if row.empty:\n",
    "        print(filename)\n",
    "    else:\n",
    "        labels.append(label)\n",
    "\n",
    "dataset = np.array(dataset) / 255.0\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "(train_X, valid_X, train_Y, valid_Y) = train_test_split(dataset, labels,\n",
    "                                                        test_size=0.2, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226bc79d",
   "metadata": {},
   "source": [
    "Let's check the accuracy of the model on the validation set we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff5bc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation dataset: 98.92%\n"
     ]
    }
   ],
   "source": [
    "pred_base = classifier.predict(valid_X)\n",
    "acc_base = np.sum(np.argmax(pred_base, axis=1) == np.argmax(valid_Y, axis=1)) / len(valid_Y)\n",
    "print(f\"Accuracy on validation dataset: {acc_base * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea2b1b0",
   "metadata": {},
   "source": [
    "Now, similar to what we did in Project 2, we will create adversarial examples using the FGSM method. In this case, though, we will be using the Cleverhans implementation of the algorithm instead of that of ART. We will also create adversarial examples by adding random uniform noise to our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fb6b323",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON_FGSM = 0.08\n",
    "EPSILON_NOISE = 0.03\n",
    "\n",
    "num_samples = 500 # as suggested in the hints\n",
    "\n",
    "# Select num_samples random entries from the validation set\n",
    "idx = np.random.randint(0, valid_X.shape[0], num_samples)\n",
    "valid_X_fgsm = fast_gradient_method(classifier, valid_X[idx, :], EPSILON_FGSM, np.inf)\n",
    "valid_Y_fgsm = valid_Y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94932e3",
   "metadata": {},
   "source": [
    "Let's compute the predictions of the classifier on the adversarial examples and calculate the accuracy on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0510efd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on fgsm-corrupted dataset: 20.80%\n"
     ]
    }
   ],
   "source": [
    "pred_fgsm = classifier.predict(valid_X_fgsm)\n",
    "acc_fgsm = np.sum(np.argmax(pred_fgsm, axis=1) == np.argmax(valid_Y_fgsm, axis=1)) / len(valid_Y_fgsm)\n",
    "print(f\"Accuracy on fgsm-corrupted dataset: {acc_fgsm * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dba74b",
   "metadata": {},
   "source": [
    "Now let's do the same adding uniform noise to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98f09720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on noise-corrupted dataset: 99.00%\n"
     ]
    }
   ],
   "source": [
    "valid_X_noise = valid_X[idx, :] + np.random.uniform(low=-EPSILON_NOISE, high=EPSILON_NOISE,\n",
    "                                                    size=valid_X[idx, :].shape)\n",
    "valid_Y_noise = valid_Y_fgsm\n",
    "pred_noise = classifier.predict(valid_X_noise)\n",
    "acc_noise = np.sum(np.argmax(pred_noise, axis=1) == np.argmax(valid_Y_noise, axis=1)) / len(valid_Y_noise)\n",
    "print(f\"Accuracy on noise-corrupted dataset: {acc_noise * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0144dcf1",
   "metadata": {},
   "source": [
    "Ok, surprisingly adding noise INCREASED the accuracy! But the point here is to show that it's not enough to add noise to the image, we have to do so in a sensible way if we want to create an adversarial example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff23c06",
   "metadata": {},
   "source": [
    "Now, let's make our classifier robust to the FGSM attack. We will do so by training our current classifier with the images from the dataset AND the adversarial images we will create in each training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2817c17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4261751da02e4580a38988139111acc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00092: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Using the same values as in the hints\n",
    "NUM_OF_RETRAIN_ITER = 3\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001, verbose=1, restore_best_weights=True,\n",
    "                   mode='min')\n",
    "\n",
    "\n",
    "for i in tqdm(range(NUM_OF_RETRAIN_ITER)):\n",
    "    idx = np.random.randint(0, train_X.shape[0], num_samples)\n",
    "    X_fgsm = fast_gradient_method(classifier, train_X[idx, :], EPSILON_FGSM, np.inf)\n",
    "    Y_fgsm = train_Y[idx]\n",
    "    \n",
    "    train_X_tmp = np.concatenate((train_X, X_fgsm), axis=0)\n",
    "    train_Y_tmp = np.concatenate((train_Y, Y_fgsm), axis=0)\n",
    "    \n",
    "    classifier.fit(train_X_tmp, train_Y_tmp, epochs=100, verbose=0, callbacks=[es],\n",
    "              validation_data=(X_fgsm, Y_fgsm), batch_size=1000)\n",
    "    \n",
    "classifier.save('../robust_chinese_traffic_sign_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ef534a",
   "metadata": {},
   "source": [
    "Ok, we can now check how the model performs on adversarial images after the training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a6c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, valid_X.shape[0], num_samples)\n",
    "valid_X_fgsm = fast_gradient_method(classifier, valid_X[idx, :], EPSILON_FGSM, np.inf)\n",
    "valid_Y_fgsm = valid_Y[idx]\n",
    "pred_fgsm = classifier.predict(valid_X_fgsm)\n",
    "acc_fgsm = np.sum(np.argmax(pred_fgsm, axis=1) == np.argmax(valid_Y_fgsm, axis=1)) / len(valid_Y_fgsm)\n",
    "print(f\"Accuracy on fgsm-corrupted dataset: {acc_fgsm * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5b9ea",
   "metadata": {},
   "source": [
    "Finally, we plot the images of the original dataset and the ones corrupted with noise and the fgsm method, as well as the prediction from the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ad985",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = 3\n",
    "COLS = 8\n",
    "fig, axes = plt.subplots(ROWS, COLS, figsize=(20, 10))\n",
    "\n",
    "row_labels = ['GT', 'Noise', 'FGSM']\n",
    "\n",
    "idx = np.random.randint(0, valid_X.shape[0], COLS)\n",
    "valid_X_subset = valid_X[idx, :]\n",
    "valid_X_fgsm = fast_gradient_method(classifier, valid_X_subset, EPSILON_FGSM, np.inf)\n",
    "valid_X_noise = valid_X_subset + np.random.uniform(low=-EPSILON_NOISE, high=EPSILON_NOISE,\n",
    "                                                    size=valid_X_subset.shape)\n",
    "\n",
    "pred_base = classifier.predict(valid_X_subset)\n",
    "pred_fgsm = classifier.predict(valid_X_fgsm)\n",
    "pred_noise = classifier.predict(valid_X_noise)\n",
    "\n",
    "for k in range(COLS):\n",
    "    axes[0,k].imshow(valid_X_subset[k])\n",
    "    axes[1,k].imshow(valid_X_noise[k].clip(0.0,1.0)) #.clip(0.0,1.0)\n",
    "    axes[2,k].imshow(valid_X_fgsm[k])\n",
    "    \n",
    "    axes[0,k].set_title(str(pred_base[k].argmax()))\n",
    "    axes[1,k].set_title(str(pred_noise[k].argmax()))\n",
    "    axes[2,k].set_title(str(pred_fgsm[k].argmax()))\n",
    "    \n",
    "for ax, row in zip(axes[:,0], row_labels):\n",
    "    ax.set_ylabel(row, rotation=30, size='large')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv",
   "language": "python",
   "name": "adv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
